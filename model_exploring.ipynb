{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65d5f6e5-18c4-4eaa-afd0-a2b1217bbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f019ce2c-298b-4447-82b7-7ac9918968c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "499f64ac-8ff0-4778-b4cb-acd190320687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "03da0a3a-25c2-47ff-847f-82dda97de228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(h5_file:str) -> Tuple[float, float]:\n",
    "    \"\"\" Extracts useful data from a .h5 file\n",
    "\n",
    "    :param h5_file: File to extract the train and val accuracy scores from\n",
    "    :return: train accuracy, difference between train and validation\n",
    "    \"\"\"\n",
    "    scores = re.findall(r\"\\d+\\.\\d+\",h5_file)\n",
    "    train_accuracy = float(scores[0])\n",
    "    val_accuracy = float(scores[1])\n",
    "    difference = train_accuracy - val_accuracy\n",
    "    return val_accuracy, difference\n",
    "\n",
    "\n",
    "\n",
    "def keep_best_saved_h5(folder_relative:str, common_filename: str, maximum_difference:float) -> str:\n",
    "    \"\"\" Goes through all common named .h5-files,\n",
    "    deletes all from folder except for the best result.\n",
    "\n",
    "    :param maximum_difference: max difference allowed between accuracy and val_accuracy.\n",
    "    :return Best scoring .h5 file\n",
    "    \"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    os.chdir(current_directory + folder_relative)\n",
    "    all_files = os.listdir()\n",
    "    best_scoring_file = \"\"\n",
    "    # try-except incase of errors: returns to current directory\n",
    "    try:\n",
    "        # Keep the files with a low difference between train_accuracy and validation_accuracy.\n",
    "        # Deletes the rest from directory\n",
    "        model_files = [file for file in all_files if file.startswith(common_filename)]\n",
    "        scores = []\n",
    "        not_overfitting_models = []\n",
    "        best_scoring_file = \"\"\n",
    "        for file in model_files:\n",
    "            validation_accuracy, diff = get_score(file) # Uses function get_score()\n",
    "            if abs(diff) > maximum_difference:\n",
    "                os.remove(file)\n",
    "            else:\n",
    "                not_overfitting_models.append(file)\n",
    "                scores.append(validation_accuracy)\n",
    "\n",
    "        # Keep only the file with highest validation accuracy score.\n",
    "        # Deletes the rest from directory\n",
    "        highest_score_index = scores.index(max(scores))\n",
    "        for i, file in enumerate(not_overfitting_models):\n",
    "            if i == highest_score_index:\n",
    "                best_scoring_file = file\n",
    "            else:\n",
    "                os.remove(file)\n",
    "        os.chdir(current_directory)\n",
    "    except:\n",
    "        os.chdir(current_directory)\n",
    "    print(f\"Currently in directory:{os.getcwd()}\")\n",
    "    print(f\"File coming out of the function: {best_scoring_file}\")\n",
    "    return best_scoring_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b6331-e1d2-4953-b9c7-059cf9944500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "72a468d8-5d33-4c64-805d-78d105f6676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train\"\n",
    "test_path = \"data/test\"\n",
    "labels = {\"akiec\":0, \"bcc\":1, \"bkl\":2, \"df\":3, \"mel\":4, \"nv\":5, \"vasc\":6}\n",
    "classes = list(labels.keys())\n",
    "size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3a3fc5ae-41a3-477c-8737-d34e84cd0e79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21622 images belonging to 7 classes.\n",
      "Found 5402 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "model_generator = ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range = 40,\n",
    "                                        width_shift_range = 0.2, height_shift_range = 0.2, rescale = 1./255,\n",
    "                                        shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,\n",
    "                                        validation_split=.2)\n",
    "\n",
    "train_batches = model_generator.flow_from_directory(train_path,target_size=(1, 28, 28, 3), classes=classes, color_mode='rgb', batch_size=10, shuffle=True, subset=\"training\")\n",
    "validation_batches = model_generator.flow_from_directory(train_path,size,classes=classes, color_mode='rgb', batch_size=20, shuffle=True, subset=\"validation\")\n",
    "test_batches = ImageDataGenerator(preprocessing_function=preprocess_input)\\\n",
    "    .flow_from_directory(test_path,size,classes=classes, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d03e969e-08e4-45af-94ae-2d0f93f5b592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer max_pooling2d_95 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, 28, 28, 3, 16)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28692/1043734580.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m   \u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFlatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m   \u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'relu'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m   \u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m7\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m ])\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    528\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, layers, name)\u001B[0m\n\u001B[0;32m    132\u001B[0m         \u001B[0mlayers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlayers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 134\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    135\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    528\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36madd\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    215\u001B[0m       \u001B[1;31m# If the model is being built continuously on top of an input layer:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    216\u001B[0m       \u001B[1;31m# refresh its output.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 217\u001B[1;33m       \u001B[0moutput_tensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    218\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    219\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    975\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_in_functional_construction_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    976\u001B[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001B[1;32m--> 977\u001B[1;33m                                                 input_list)\n\u001B[0m\u001B[0;32m    978\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    979\u001B[0m     \u001B[1;31m# Maintains info about the `Layer.call` stack.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_functional_construction_call\u001B[1;34m(self, inputs, args, kwargs, input_list)\u001B[0m\n\u001B[0;32m   1113\u001B[0m       \u001B[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1114\u001B[0m       outputs = self._keras_tensor_symbolic_call(\n\u001B[1;32m-> 1115\u001B[1;33m           inputs, input_masks, args, kwargs)\n\u001B[0m\u001B[0;32m   1116\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1117\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0moutputs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_keras_tensor_symbolic_call\u001B[1;34m(self, inputs, input_masks, args, kwargs)\u001B[0m\n\u001B[0;32m    846\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeras_tensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mKerasTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_signature\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    847\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 848\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_infer_output_signature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    850\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_infer_output_signature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_infer_output_signature\u001B[1;34m(self, inputs, args, kwargs, input_masks)\u001B[0m\n\u001B[0;32m    884\u001B[0m           \u001B[1;31m# overridden).\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    885\u001B[0m           \u001B[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 886\u001B[1;33m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_build\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    887\u001B[0m           \u001B[0minputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_cast_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_maybe_build\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   2632\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2633\u001B[0m       input_spec.assert_input_compatibility(\n\u001B[1;32m-> 2634\u001B[1;33m           self.input_spec, inputs, self.name)\n\u001B[0m\u001B[0;32m   2635\u001B[0m       \u001B[0minput_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2636\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0minput_list\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dtype_policy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_dtype\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\input_spec.py\u001B[0m in \u001B[0;36massert_input_compatibility\u001B[1;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[0;32m    216\u001B[0m                          \u001B[1;34m'expected ndim='\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m', found ndim='\u001B[0m \u001B[1;33m+\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m                          \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m'. Full shape received: '\u001B[0m \u001B[1;33m+\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 218\u001B[1;33m                          str(tuple(shape)))\n\u001B[0m\u001B[0;32m    219\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_ndim\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m       \u001B[0mndim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrank\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Input 0 of layer max_pooling2d_95 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, 28, 28, 3, 16)"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(224, 224, 3, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(7)\n",
    "])\n",
    "  \n",
    "model_extended_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b4b876f2-fa78-456b-8ddc-31025fc8a047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1,28,3) into shape (1,28,28,3,3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28692/167970967.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m               metrics=['accuracy'])\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mmodel_extended_2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_batches\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_batches\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m16\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1146\u001B[0m           \u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1147\u001B[0m           \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1148\u001B[1;33m           steps_per_execution=self._steps_per_execution)\n\u001B[0m\u001B[0;32m   1149\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1150\u001B[0m       \u001B[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36mget_data_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1381\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"model\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"_cluster_coordinator\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1382\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0m_ClusterCoordinatorDataHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1383\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mDataHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1384\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1385\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[0;32m   1148\u001B[0m         \u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m         \u001B[0mdistribution_strategy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1150\u001B[1;33m         model=model)\n\u001B[0m\u001B[0;32m   1151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1152\u001B[0m     \u001B[0mstrategy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001B[0m\n\u001B[0;32m    922\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    923\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 924\u001B[1;33m         **kwargs)\n\u001B[0m\u001B[0;32m    925\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    926\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001B[0m\n\u001B[0;32m    792\u001B[0m     \u001B[1;31m# Since we have to know the dtype of the python generator when we build the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    793\u001B[0m     \u001B[1;31m# dataset, we have to look at a batch to infer the structure.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 794\u001B[1;33m     \u001B[0mpeek\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_peek_and_restore\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    795\u001B[0m     \u001B[0mpeek\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_standardize_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpeek\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m     \u001B[0mpeek\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_tensorlike\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpeek\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m_peek_and_restore\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    926\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    927\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_peek_and_restore\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 928\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    929\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    930\u001B[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     63\u001B[0m         index_array = self.index_array[self.batch_size * idx:\n\u001B[0;32m     64\u001B[0m                                        self.batch_size * (idx + 1)]\n\u001B[1;32m---> 65\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_batches_of_transformed_samples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex_array\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001B[0m in \u001B[0;36m_get_batches_of_transformed_samples\u001B[1;34m(self, index_array)\u001B[0m\n\u001B[0;32m    238\u001B[0m                 \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_data_generator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    239\u001B[0m                 \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_data_generator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstandardize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 240\u001B[1;33m             \u001B[0mbatch_x\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    241\u001B[0m         \u001B[1;31m# optionally save augmented images to disk for debugging purposes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_to_dir\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: could not broadcast input array from shape (1,28,3) into shape (1,28,28,3,3)"
     ]
    }
   ],
   "source": [
    "# Compile and fit the model (10 epochs too)\n",
    "model_extended_2.compile(loss=tf.keras.losses.CategoricalCrossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_extended_2.fit(train_batches, epochs=10,  validation_data=validation_batches, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf759a3-607c-4262-b8cb-c77c17be780e",
   "metadata": {},
   "source": [
    "# model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "652936a4-961a-4dea-a413-20b69bb81d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_84 (Conv2D)           (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 2, 2, 256)         819456    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 1,046,151\n",
      "Trainable params: 1,046,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the final version of the model\n",
    "# Create the second version of the model\n",
    "# pixel width and height of our images\n",
    "\n",
    "# number of filters in the convnet layer\n",
    "filters = 64\n",
    "\n",
    "# conv net parameters\n",
    "strides = (2, 2)\n",
    "pool_size = (2,2)\n",
    "kernel_size = (5, 5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=kernel_size, strides=strides, activation='relu', padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(filters=256, kernel_size=kernel_size, strides=strides, activation='relu', padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "55832291-3e10-4ed4-901f-c76ef9b1d9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (32,32,3) into shape (32,32,3,3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28692/1096252882.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m#lrl = ReduceLROnPlateau(patience=10,verbose=1,monitor=\"loss\")\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mmodel_costumized\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_batches\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_batches\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m16\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmc\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1146\u001B[0m           \u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1147\u001B[0m           \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1148\u001B[1;33m           steps_per_execution=self._steps_per_execution)\n\u001B[0m\u001B[0;32m   1149\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1150\u001B[0m       \u001B[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36mget_data_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1381\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"model\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"_cluster_coordinator\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1382\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0m_ClusterCoordinatorDataHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1383\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mDataHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1384\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1385\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[0;32m   1148\u001B[0m         \u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m         \u001B[0mdistribution_strategy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1150\u001B[1;33m         model=model)\n\u001B[0m\u001B[0;32m   1151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1152\u001B[0m     \u001B[0mstrategy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistribute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001B[0m\n\u001B[0;32m    922\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    923\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 924\u001B[1;33m         **kwargs)\n\u001B[0m\u001B[0;32m    925\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    926\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001B[0m\n\u001B[0;32m    792\u001B[0m     \u001B[1;31m# Since we have to know the dtype of the python generator when we build the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    793\u001B[0m     \u001B[1;31m# dataset, we have to look at a batch to infer the structure.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 794\u001B[1;33m     \u001B[0mpeek\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_peek_and_restore\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    795\u001B[0m     \u001B[0mpeek\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_standardize_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpeek\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m     \u001B[0mpeek\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_tensorlike\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpeek\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m_peek_and_restore\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    926\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    927\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_peek_and_restore\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 928\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    929\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    930\u001B[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     63\u001B[0m         index_array = self.index_array[self.batch_size * idx:\n\u001B[0;32m     64\u001B[0m                                        self.batch_size * (idx + 1)]\n\u001B[1;32m---> 65\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_batches_of_transformed_samples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex_array\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001B[0m in \u001B[0;36m_get_batches_of_transformed_samples\u001B[1;34m(self, index_array)\u001B[0m\n\u001B[0;32m    238\u001B[0m                 \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_data_generator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    239\u001B[0m                 \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage_data_generator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstandardize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 240\u001B[1;33m             \u001B[0mbatch_x\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    241\u001B[0m         \u001B[1;31m# optionally save augmented images to disk for debugging purposes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_to_dir\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: could not broadcast input array from shape (32,32,3) into shape (32,32,3,3)"
     ]
    }
   ],
   "source": [
    "# Compile and fit the model (10 epochs too)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_name = 'model_costumized'\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "mc = ModelCheckpoint('deployment/models/'+model_name+'training{accuracy:.4f}_{val_accuracy:.4f}.h5',\n",
    "                     monitor='val_accuracy', mode='max', verbose=1,\n",
    "                     save_best_only=True)\n",
    "#lrl = ReduceLROnPlateau(patience=10,verbose=1,monitor=\"loss\")\n",
    "\n",
    "model_costumized = model.fit(train_batches, epochs=10,  validation_data=validation_batches, batch_size=16, verbose=1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0041e476-1da6-49bf-a11c-4a6bd97d280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently in directory:C:\\Users\\leono\\Documents\\python_projects\\challenge-mole\\challenge-mole\n",
      "File coming out of the function: model_costumizedtraining0.4894_0.4828.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = keep_best_saved_h5(\"/deployment/models/\",model_name,0.02)\n",
    "model = load_model(f\"deployment/models/{best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddb9ca-0e43-4f2f-9ced-dab13af8101a",
   "metadata": {},
   "source": [
    "# Mobile_net different pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49e6f628-5609-418d-9422-1542975af58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_preprocessed_images(images_directory: str, image_size: tuple) -> list:\n",
    "    images = []\n",
    "    for img in os.listdir(images_directory):\n",
    "        img = image.load_img(images_directory+img, target_size=image_size)\n",
    "        img = image.img_to_array(img)\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "        img = preprocess_input(img)\n",
    "        images.append(img)\n",
    "    return np.vstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f96b6e16-002f-4148-bcf6-ae003aa4f176",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataHAM10000_metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28692/4246015443.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# Load your images and preprocess them.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m#akiec_images = get_preprocessed_images(\"data/train/akiec\", image_size)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mcancer_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_preprocessed_images\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"data\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;31m#bkl_images = get_preprocessed_images(\"data/train/bkl\", image_size)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;31m#df_images = get_preprocessed_images(\"data/train/df\", image_size)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_28692/4047459865.py\u001B[0m in \u001B[0;36mget_preprocessed_images\u001B[1;34m(images_directory, image_size)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mimages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mimg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages_directory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_img\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages_directory\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mimage_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimg_to_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\preprocessing\\image.py\u001B[0m in \u001B[0;36mload_img\u001B[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001B[0m\n\u001B[0;32m    312\u001B[0m   \"\"\"\n\u001B[0;32m    313\u001B[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001B[1;32m--> 314\u001B[1;33m                         target_size=target_size, interpolation=interpolation)\n\u001B[0m\u001B[0;32m    315\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001B[0m in \u001B[0;36mload_img\u001B[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001B[0m\n\u001B[0;32m    111\u001B[0m         raise ImportError('Could not import PIL.Image. '\n\u001B[0;32m    112\u001B[0m                           'The use of `load_img` requires PIL.')\n\u001B[1;32m--> 113\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpil_image\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBytesIO\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcolor_mode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'grayscale'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'dataHAM10000_metadata.csv'"
     ]
    }
   ],
   "source": [
    "# Import the keras preprocessing method.\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "image_size = (224, 224)\n",
    "# Load your images and preprocess them.\n",
    "#akiec_images = get_preprocessed_images(\"data/train/akiec\", image_size)\n",
    "cancer_images = get_preprocessed_images(\"data\", image_size)\n",
    "#bkl_images = get_preprocessed_images(\"data/train/bkl\", image_size)\n",
    "#df_images = get_preprocessed_images(\"data/train/df\", image_size)\n",
    "#mel_images = get_preprocessed_images(\"data/train/mel\", image_size)\n",
    "#nv_images = get_preprocessed_images(\"data/train/nv\", image_size)\n",
    "#vasc_images = get_preprocessed_images(\"data/train/vasc\", image_size)\n",
    "\n",
    "# Make a numpy array for each of the class labels (one hot encoded).\n",
    "akiec_labels = np.tile([1, 0], (akiec_images.shape[0], 1))\n",
    "bcc_labels = np.tile([0, 1], (bcc_images.shape[0], 1))\n",
    "bkl_labels = np.tile([0, 1], (bkl_images.shape[0], 1))\n",
    "df_labels = np.tile([0, 1], (df_images.shape[0], 1))\n",
    "mel_labels = np.tile([0, 1], (mel_labels.shape[0], 1))\n",
    "nv_labels = np.tile([0, 1], (nv_labels.shape[0], 1))\n",
    "vasc_labels = np.tile([0, 1], (vasc_labels.shape[0], 1))\n",
    "\n",
    "# Concatenate your images and your labels into X and y.\n",
    "X = np.concatenate([akiec_images, bcc_images, bkl_images, df_images, mel_images, nv_images, vasc_images])\n",
    "y = np.concatenate([akiec_labels, bcc_labels, bkl_labels, df_labels, mel_labels, nv_labels, vasc_labels])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec11b7-0c81-41b5-9b54-0e63a43d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, \n",
    "    y_train_val,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248111dd-867f-44b6-b742-d18a9bdaa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Determine the number of generated samples you want per original sample.\n",
    "datagen_batch_size = 16\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "train_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your train data.\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=datagen_batch_size)\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "validation_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                        horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your validation data.\n",
    "validation_generator = validation_datagen.flow(X_val, y_val, batch_size=datagen_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f1a87-6881-43d9-9e59-b96931abf590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your chosen model!\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "# Make a model object. \n",
    "# Make sure you exclude the top part. set the input shape of the model to 224x224 pixels, with 3 color channels.\n",
    "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c498e-9f58-431d-b7b5-76677f4d84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(64, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469278d-9d57-478e-9cec-418994403756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss. \n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "new_model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "history = new_model.fit(train_generator,\n",
    "                        epochs=10, \n",
    "                        batch_size=8,\n",
    "                        validation_data=validation_generator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1f6d4-8a0b-4d4d-bc10-958ac6a728d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history : tensorflow.keras.callbacks.History):\n",
    "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
    "    that is output from your `fit` method to plot the loss and accuracy of\n",
    "    the training and validation set.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "    axs[0].plot(history.history['accuracy'], label='training set')\n",
    "    axs[0].plot(history.history['val_accuracy'], label = 'validation set')\n",
    "    axs[0].set(xlabel = 'Epoch', ylabel='Accuracy', ylim=[0, 1])\n",
    "\n",
    "    axs[1].plot(history.history['loss'], label='training set')\n",
    "    axs[1].plot(history.history['val_loss'], label = 'validation set')\n",
    "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 10])\n",
    "    \n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[1].legend(loc='lower right')\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fec9aa5-c9e1-433b-91fc-93eb40978415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobile_net same pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "731b6d3c-f21f-4bfc-abf9-2f902835193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21622 images belonging to 7 classes.\n",
      "Found 5402 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "model_generator_mobile_net = ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range = 40,\n",
    "                                        width_shift_range = 0.2, height_shift_range = 0.2, rescale = 1./255,\n",
    "                                        shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,\n",
    "                                        validation_split=.2)\n",
    "\n",
    "train_batches_mobile_net = model_generator_mobile_net.flow_from_directory(train_path,target_size=size,classes=classes,\n",
    "                                                batch_size=10, shuffle=True, subset=\"training\")\n",
    "validation_batches_mobile_net = model_generator_mobile_net.flow_from_directory(train_path,size,classes=classes,\n",
    "                                                batch_size=20, shuffle=True, subset=\"validation\")\n",
    "test_batches_mobile_net = ImageDataGenerator(preprocessing_function=preprocess_input)\\\n",
    "    .flow_from_directory(test_path,size,classes=classes, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b323ea7a-d1a8-4012-ba27-12933a0e4500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 7)                 628231    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 637,966\n",
      "Trainable params: 637,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(64, activation='relu'))\n",
    "new_model.add(Dropout(0.3))\n",
    "new_model.add(Dense(128, activation='relu'))\n",
    "new_model.add(Dropout(0.3))\n",
    "new_model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19961e33-bad6-4081-8bf3-3835838ccdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2163/2163 [==============================] - 216s 100ms/step - loss: 1.2631 - accuracy: 0.5000 - val_loss: 1.3437 - val_accuracy: 0.4793\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.47927, saving model to deployment/models\\model_costumized_mobile_nettraining0.5000_0.4793.h5\n",
      "Epoch 2/10\n",
      "2163/2163 [==============================] - 211s 98ms/step - loss: 1.2445 - accuracy: 0.5094 - val_loss: 1.3102 - val_accuracy: 0.4861\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.47927 to 0.48612, saving model to deployment/models\\model_costumized_mobile_nettraining0.5094_0.4861.h5\n",
      "Epoch 3/10\n",
      "2163/2163 [==============================] - 209s 97ms/step - loss: 1.2177 - accuracy: 0.5195 - val_loss: 1.3248 - val_accuracy: 0.4606\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.48612\n",
      "Epoch 4/10\n",
      "2163/2163 [==============================] - 206s 95ms/step - loss: 1.1953 - accuracy: 0.5300 - val_loss: 1.3320 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.48612\n",
      "Epoch 5/10\n",
      "2163/2163 [==============================] - 207s 96ms/step - loss: 1.1726 - accuracy: 0.5433 - val_loss: 1.2228 - val_accuracy: 0.5126\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.48612 to 0.51259, saving model to deployment/models\\model_costumized_mobile_nettraining0.5433_0.5126.h5\n",
      "Epoch 6/10\n",
      "2163/2163 [==============================] - 202s 94ms/step - loss: 1.1625 - accuracy: 0.5460 - val_loss: 1.2620 - val_accuracy: 0.4885\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.51259\n",
      "Epoch 7/10\n",
      "2163/2163 [==============================] - 202s 93ms/step - loss: 1.1440 - accuracy: 0.5571 - val_loss: 1.3543 - val_accuracy: 0.4484\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.51259\n",
      "Epoch 8/10\n",
      "2163/2163 [==============================] - 213s 99ms/step - loss: 1.1327 - accuracy: 0.5573 - val_loss: 1.3450 - val_accuracy: 0.4639\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.51259\n",
      "Epoch 9/10\n",
      "2163/2163 [==============================] - 206s 95ms/step - loss: 1.1126 - accuracy: 0.5673 - val_loss: 1.3510 - val_accuracy: 0.4663\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.51259\n",
      "Epoch 10/10\n",
      "2163/2163 [==============================] - 202s 93ms/step - loss: 1.1053 - accuracy: 0.5696 - val_loss: 1.2926 - val_accuracy: 0.4830\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.51259\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit the model (10 epochs too)\n",
    "new_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_name = 'model_costumized_mobile_net'\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "mc = ModelCheckpoint('deployment/models/'+model_name+'training{accuracy:.4f}_{val_accuracy:.4f}.h5',\n",
    "                     monitor='val_accuracy', mode='max', verbose=1,\n",
    "                     save_best_only=True)\n",
    "#lrl = ReduceLROnPlateau(patience=10,verbose=1,monitor=\"loss\")\n",
    "\n",
    "model_costumized_mobile_net = model.fit(train_batches, epochs=10,  validation_data=validation_batches, batch_size=16, verbose=1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9736e-1d53-470d-bdb2-229f12313b40",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
